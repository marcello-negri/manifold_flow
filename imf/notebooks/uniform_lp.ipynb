{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import tqdm\n",
    "from enflows.distributions import StandardNormal, Uniform, DiagonalNormal, MADEMoG, MOG\n",
    "from enflows.transforms import Sigmoid, ScalarScale, ScalarShift, RandomPermutation, MaskedSumOfSigmoidsTransform, ConditionalSumOfSigmoidsTransform\n",
    "from enflows.transforms.normalization import ActNorm\n",
    "from enflows.transforms.base import CompositeTransform, InverseTransform\n",
    "from enflows.transforms.lipschitz import LipschitzDenseNetBuilder, iResBlock\n",
    "from enflows.nn.nets.activations import Sin\n",
    "from enflows.nn.nets import ResidualNet\n",
    "from enflows.flows.base import Flow\n",
    "from enflows.transforms.injective import LpManifoldFlow, ConstrainedAnglesSigmoid, ClampedAngles, FixedNorm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:02:25.757904763Z",
     "start_time": "2023-12-04T16:02:25.641875597Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T15:48:58.652075025Z",
     "start_time": "2023-12-04T15:48:58.577107813Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def set_random_seeds (seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def uniform_p_norm (beta):\n",
    "    return torch.ones_like(beta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T15:48:58.996574173Z",
     "start_time": "2023-12-04T15:48:58.949204908Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def spherical_to_cartesian_torch(arr):\n",
    "    # meant for batches of vectors, i.e. arr.shape = (mb, n)\n",
    "    \n",
    "    assert arr.shape[1] >= 2\n",
    "    r = arr[:, :1]\n",
    "    angles = arr[:, 1:]\n",
    "    \n",
    "    sin_prods = torch.cumprod(torch.sin(angles), dim=1)\n",
    "    x1 = r * torch.cos(angles[:, :1])\n",
    "    xs = r * sin_prods[:, :-1] * torch.cos(angles[:, 1:])\n",
    "    xn = r * sin_prods[:, -1:]\n",
    "\n",
    "    return torch.cat((x1, xs, xn), dim=1)\n",
    "\n",
    "def cartesian_to_spherical_torch(arr):\n",
    "    # meant for batches of vectors, i.e. arr.shape = (mb, n)\n",
    "    eps = 1e-5\n",
    "    assert arr.shape[-1] >= 2\n",
    "    radius = torch.linalg.norm(arr, dim=-1)\n",
    "    flipped_cumsum = torch.cumsum(torch.flip(arr ** 2, dims=(-1,)), dim=-1)\n",
    "    sqrt_sums = torch.flip(torch.sqrt(flipped_cumsum + eps), dims=(-1,))[...,:-1]\n",
    "    angles = torch.acos(arr[..., :-1] / (sqrt_sums + eps))\n",
    "    last_angle = ((arr[...,-1] >= 0).float() * angles[..., -1] + \\\n",
    "                (arr[...,-1] < 0).float() * (2 * np.pi - angles[..., -1]))\n",
    "\n",
    "    return torch.cat((radius.unsqueeze(-1), angles[..., :-1], last_angle.unsqueeze(-1)), dim=-1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T15:48:59.471383969Z",
     "start_time": "2023-12-04T15:48:59.428502520Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# def log_prior_angles (beta:torch.Tensor, sigma:torch.Tensor):\n",
    "#     eps = 1e-7\n",
    "#     beta_spherical = cartesian_to_spherical_torch(beta)[...,1:]\n",
    "#     theta_0_pi = (beta_spherical).square().sum(-1) / (sigma**2 + eps)\n",
    "#     theta_pi__2 = (beta_spherical - 0.5 * np.pi).square().sum(-1) / (sigma**2 + eps)\n",
    "#     theta_pi = (beta_spherical - np.pi).square().sum(-1) / (sigma**2 + eps)\n",
    "# \n",
    "#     # theta_3_pi__2 = (beta_spherical[...,-1:] - 1.5 * np.pi).square().sum(-1) / (sigma**2 + eps)\n",
    "#     # theta_2_pi = (beta_spherical[...,-1:] - 2 * np.pi).square().sum(-1) / (sigma**2 + eps)\n",
    "# \n",
    "#     # prior_theta = - 0.5 * (theta_0_pi + theta_pi__2 + theta_pi)# + theta_3_pi__2 + theta_2_pi)\n",
    "#     prior_theta = - 0.5 * (theta_0_pi + theta_pi)# + theta_3_pi__2 + theta_2_pi)\n",
    "# \n",
    "#     return prior_theta\n",
    "\n",
    "def log_prior_angles (beta:torch.Tensor, sigma:torch.Tensor):\n",
    "    beta_spherical = cartesian_to_spherical_torch(beta)[...,1:]\n",
    "    exp = []\n",
    "    angles = [0.5 * np.pi]\n",
    "    #angles = [0., 0.5*np.pi, 1.*np.pi, 1.5*np.pi, 2*np.pi]\n",
    "    for mean in angles:\n",
    "        #exp.append(normal_exp(beta_spherical[...,:-1], mean, sigma))\n",
    "        #print(\"shape exp\", normal_exp(beta_spherical[...,:-1], mean, sigma).shape)\n",
    "        exp.append(normal_exp(beta_spherical[...,:-1], mean, sigma))\n",
    "        print(\"shape exp\", normal_exp(beta_spherical[...,:-1], mean, sigma).shape)\n",
    "    angles = [1.5 * np.pi]\n",
    "    #angles = [0., 0.5*np.pi, 1.*np.pi, 1.5*np.pi, 2*np.pi]\n",
    "    for mean in angles:\n",
    "        #exp.append(normal_exp(beta_spherical[...,:-1], mean, sigma))\n",
    "        #print(\"shape exp\", normal_exp(beta_spherical[...,:-1], mean, sigma).shape)\n",
    "        exp.append(normal_exp(beta_spherical[...,-1:], mean, sigma))\n",
    "        print(\"shape exp\", normal_exp(beta_spherical[...,-1:], mean, sigma).shape)\n",
    "    exp_torch = torch.cat(exp, dim=-1)\n",
    "    # print(\"shape beta\", beta_spherical.shape)\n",
    "    # print(\"shape exp_torch\", exp_torch.shape)\n",
    "    # print(\"shape log_sum_exp\", torch.logsumexp(exp_torch, dim=-1).shape)\n",
    "    \n",
    "    # const = torch.tensor(2*np.pi * sigma * len(angles) * beta_spherical[...,:-1].shape[-1])\n",
    "    # const = torch.tensor(2*np.pi * sigma * len(angles) * beta_spherical.shape[-1])\n",
    "    # const = torch.tensor(2*np.pi * sigma * len(angles))\n",
    "    n_dim = beta_spherical[...,:-1].shape[-1]\n",
    "    const = torch.tensor(2*np.pi * (sigma**2))\n",
    "    return torch.logsumexp(exp_torch, dim=-1) - 0.5 * n_dim * torch.log(const) - torch.log(torch.tensor(len(angles)))\n",
    "                   \n",
    "    # theta_00_pi = torch.logsumexp( - 0.5 * (beta_spherical[...,:-1] / sigma ).square(), dim=-1) \n",
    "    # theta_05_pi = torch.logsumexp( - 0.5 * ((beta_spherical[...,:-1] - 0.5*np.pi) / sigma).square(), dim=-1) \n",
    "    # theta_10_pi = torch.logsumexp( - 0.5 * ((beta_spherical[...,:-1] - 1.0*np.pi) / sigma).square(), dim=-1)\n",
    "    # \n",
    "    # #theta_15_pi = torch.logsumexp( - 0.5 * ((beta_spherical[...,-1:] - 1.5*np.pi) / (sigma + eps)).square(), dim=-1)\n",
    "    # #theta_20_pi = torch.logsumexp( - 0.5 * ((beta_spherical[...,-1:] - 2.0*np.pi) / (sigma + eps)).square(), dim=-1)\n",
    "    # \n",
    "    # prior_theta = theta_00_pi + theta_05_pi + theta_10_pi #+ theta_15_pi + theta_20_pi\n",
    "\n",
    "    # return prior_theta\n",
    "\n",
    "# def normal_exp (arr, mean, sigma):\n",
    "#     return - 0.5 * ((arr - mean) / sigma ).square()\n",
    "    \n",
    "def normal_exp (arr, mean, sigma):\n",
    "    return - 0.5 * ((arr - mean) / sigma ).square().sum(-1).unsqueeze(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T15:49:00.896292249Z",
     "start_time": "2023-12-04T15:49:00.858083086Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def build_flow_manifold (flow_dim, norm, q, n_layers=3, hidden_features=256, device='cuda'):\n",
    "    # base distribution over flattened triangular matrix\n",
    "    # base_dist = StandardNormal(shape=[flow_dim-1])\n",
    "    \n",
    "    means =  torch.tensor([[1.],[-1]]).repeat(1,flow_dim-1).to(device)\n",
    "    stds = (torch.ones(means.shape[0],flow_dim-1) * 0.3).to(device)\n",
    "    base_dist = MOG(means=means, stds=stds)\n",
    "    \n",
    "    # low = torch.tensor(-1.).to(device)\n",
    "    # high = torch.tensor(1.).to(device)\n",
    "    # base_dist = Uniform(shape=[flow_dim-1], low=low, high=high)\n",
    "\n",
    "    # Define an invertible transformation\n",
    "    transformation_layers = []\n",
    "\n",
    "    for _ in range(n_layers):\n",
    "        #transformation_layers.append(RandomPermutation(features=flow_dim-1))\n",
    "\n",
    "        transformation_layers.append(\n",
    "            InverseTransform(\n",
    "                MaskedSumOfSigmoidsTransform(features=flow_dim-1, hidden_features=hidden_features, num_blocks=3, n_sigmoids=30)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #transformation_layers.append(\n",
    "        #InverseTransform(\n",
    "        #        Sigmoid()\n",
    "        #    )\n",
    "        #)\n",
    "        \n",
    "        transformation_layers.append(\n",
    "          InverseTransform(\n",
    "              ActNorm(features=flow_dim-1)\n",
    "          )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # transformation_layers.append(\n",
    "    #    InverseTransform(\n",
    "    #            CompositeTransform([\n",
    "    #                ScalarScale(scale=2, trainable=False)])#,\n",
    "    #                #ScalarShift(shift=-1, trainable=False)])\n",
    "    #        )\n",
    "    # )\n",
    "\n",
    "    #transformation_layers.append(\n",
    "    #   InverseTransform(\n",
    "    #       Sigmoid()\n",
    "    #   )\n",
    "    #)\n",
    "\n",
    "    transformation_layers.append(\n",
    "        InverseTransform(\n",
    "                ConstrainedAnglesSigmoid(temperature=1, learn_temperature=False)\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    transformation_layers.append(\n",
    "        InverseTransform(\n",
    "            ClampedAngles(eps=1e-5)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    transformation_layers.append(\n",
    "        InverseTransform(\n",
    "            LpManifoldFlow(norm=norm, p=q)\n",
    "            # FixedNorm(norm=norm, q=q)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    transformation_layers = transformation_layers[::-1]\n",
    "    transform = CompositeTransform(transformation_layers)\n",
    "\n",
    "    # combine into a flow\n",
    "    flow = Flow(transform, base_dist).to(device)\n",
    "\n",
    "    return flow\n",
    "\n",
    "def build_flow_lipshitz_manifold (flow_dim, norm, q, n_layers=3, hidden_features=256, device='cuda'):\n",
    "    # base distribution over flattened triangular matrix\n",
    "    base_dist = StandardNormal(shape=[flow_dim-1])\n",
    "    #low = torch.tensor(-1.).to(device)\n",
    "    #high = torch.tensor(1.).to(device)\n",
    "    #base_dist = Uniform(shape=[flow_dim-1], low=low, high=high)\n",
    "\n",
    "    densenet_builder = LipschitzDenseNetBuilder(input_channels=flow_dim-1,\n",
    "                                                densenet_depth=5,\n",
    "                                                activation_function=Sin(w0=30),\n",
    "                                                lip_coeff=.97,\n",
    "                                                )\n",
    "\n",
    "    transform_layers = []\n",
    "    for i in range(n_layers):\n",
    "\n",
    "        transform_layers.append(InverseTransform(iResBlock(densenet_builder.build_network(), brute_force=False)))\n",
    "        transform_layers.append(InverseTransform(ActNorm(features=flow_dim-1)))\n",
    "    \n",
    "    transform_layers.append(\n",
    "        InverseTransform(\n",
    "                ConstrainedAnglesSigmoid()\n",
    "            )\n",
    "    )\n",
    "    transform_layers.append(\n",
    "        InverseTransform(\n",
    "            FixedNorm(norm=norm, q=q)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    transform_layers = transform_layers[::-1]\n",
    "    transform = CompositeTransform(transform_layers)\n",
    "\n",
    "    flow = Flow(transform, base_dist).to(device)\n",
    "\n",
    "    return flow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:02:44.478864255Z",
     "start_time": "2023-12-04T16:02:44.435325282Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def train_model (model, epochs=2_001, lr=1e-3, sample_size=1, device=\"cuda\", **kwargs):\n",
    "\n",
    "    # optimizer = torch.optim.Adam([{'params':model.parameters()}, {'params':log_sigma, 'lr':1e-2}], lr=lr)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "    loss = []\n",
    "    try:\n",
    "        start_time = time.monotonic()\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            q_samples, q_log_prob = model.sample_and_log_prob(num_samples=sample_size)\n",
    "            if torch.any(torch.isnan(q_samples)): breakpoint()\n",
    "            \n",
    "            # log_lik = uniform_p_norm(beta=q_log_prob_beta)\n",
    "            # kl_div = torch.mean(q_log_prob_beta - log_lik)\n",
    "            # kl_div.backward()\n",
    "\n",
    "            #assert not torch.any(torch.isnan(q_log_prob))\n",
    "\n",
    "            log_prior = log_prior_angles(q_samples, torch.tensor(0.5))\n",
    "            assert not torch.any(torch.isnan(log_prior))\n",
    "            kl_div = torch.mean(q_log_prob - log_prior)\n",
    "            #kl_div = torch.mean(q_log_prob)\n",
    "            kl_div.backward()\n",
    "            \n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), .001)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss.append(torch.mean(q_log_prob_beta - log_lik).cpu().detach().numpy())\n",
    "            #loss.append(torch.mean(q_log_prob).cpu().detach().numpy())\n",
    "            loss.append(torch.mean(q_log_prob - log_prior).cpu().detach().numpy())\n",
    "\n",
    "            print(f\"Training loss at step {epoch}: {loss[-1]:.4f}\")\n",
    "            if epoch % 25 == 0:\n",
    "                \n",
    "                q_log_prob_np = q_log_prob.detach().cpu().numpy().ravel()\n",
    "                q_samples_np = q_samples.detach().cpu().numpy()\n",
    "                samples_spherical = cartesian_to_spherical_torch(torch.tensor(q_samples_np)).numpy()\n",
    "                for i in range(1, samples_spherical.shape[1]):\n",
    "                    # plt.figure(figsize=(10,5))\n",
    "                    # plt.scatter(samples_spherical[:,i], q_log_prob_np, marker='.')\n",
    "                    # plt.show()\n",
    "                    # plt.clf()\n",
    "\n",
    "                    plt.figure(figsize=(10,5))\n",
    "                    plt.hist(samples_spherical[:,i], bins=25)\n",
    "                    #n, bins, patches = plt.hist(samples_spherical[:,i], bins=25)\n",
    "                    #plt.vlines([0, np.pi * 0.5, np.pi, np.pi*1.5, 2 * np.pi], ymin=0, ymax=max(n), linestyles='dashed', colors='b')\n",
    "                    # plt.savefig(f\"{dir_name}samples_p{q}_theta_flow.pdf\", dpi=100)\n",
    "                    plt.xlim(0, 2*np.pi)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupted..\")\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "    time_diff = timedelta(seconds=end_time - start_time)\n",
    "    print(f\"Training took {time_diff} seconds\")\n",
    "\n",
    "    return model, loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:02:47.076973446Z",
     "start_time": "2023-12-04T16:02:47.024177657Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def sample_beta (model, sample_size=100, n_iter=500, device='cuda'):\n",
    "    # Sample from approximate posterior & estimate significant edges via  posterior credible interval\n",
    "    samples = []\n",
    "    for _ in tqdm.tqdm(range(n_iter)):\n",
    "        posterior_samples, log_probs_samples = model.sample_and_log_prob(sample_size)\n",
    "        samples.append(posterior_samples.cpu().detach().numpy())\n",
    "\n",
    "    return np.concatenate(samples, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:02:47.878740184Z",
     "start_time": "2023-12-04T16:02:47.838166740Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import os\n",
    "dir_name = \"./plots/\"\n",
    "if not os.path.exists(dir_name):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(dir_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:02:48.435955533Z",
     "start_time": "2023-12-04T16:02:48.393587041Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "device = 'cuda'\n",
    "set_random_seeds(123)\n",
    "\n",
    "# Build Model\n",
    "flow_dim = 5\n",
    "q = 1\n",
    "norm = 1\n",
    "flow = build_flow_manifold(flow_dim, norm=norm, q=q, n_layers=3, hidden_features=64, device=device)\n",
    "#flow = build_flow_lipshitz_manifold(flow_dim, norm=norm, q=q, n_layers=3, hidden_features=64, device=device)\n",
    "\n",
    "params = dict(lr=1e-3,\n",
    "              epochs=201,\n",
    "              sample_size=1_000,\n",
    "              device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:02:51.349795839Z",
     "start_time": "2023-12-04T16:02:51.245006886Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "flow, loss = train_model(flow, **params)\n",
    "flow.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:03:09.233839089Z",
     "start_time": "2023-12-04T16:02:53.758125054Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(len(loss)), loss)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:03:17.391707391Z",
     "start_time": "2023-12-04T16:03:17.225340107Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "n_samples = 10_000# 50_000\n",
    "n_samples_plot = 1000\n",
    "samples = sample_beta (flow, sample_size=n_samples//20, n_iter=20, device=device)\n",
    "samples_reshaped = samples.reshape(-1, flow_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:03:24.382253381Z",
     "start_time": "2023-12-04T16:03:23.993245916Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "samples_spherical = cartesian_to_spherical_torch(torch.tensor(samples)).numpy()\n",
    "for i in range(1, samples_spherical.shape[1]):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    #n, bins, patches = plt.hist(samples_spherical[:,i], bins=50)\n",
    "    plt.hist(samples_spherical[:,i], bins=50)\n",
    "    plt.vlines([0, np.pi * 0.5, np.pi, np.pi*1.5, 2 * np.pi], ymin=0, ymax=400, linestyles='dashed', colors='b', edgecolor = \"none\")\n",
    "    #plt.savefig(f\"{dir_name}samples_p{q}_theta_flow_prior_angles_{i}.pdf\", dpi=100)\n",
    "    plt.xlim(0, 2*np.pi)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:03:30.324725859Z",
     "start_time": "2023-12-04T16:03:29.570842630Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "samples_spherical = cartesian_to_spherical_torch(torch.tensor(samples)).numpy()\n",
    "for i in range(1, samples_spherical.shape[1]):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    #n, bins, patches = plt.hist(samples_spherical[:,i], bins=50)\n",
    "    plt.hist(samples_spherical[:,i], bins=50)\n",
    "    plt.vlines([0, np.pi * 0.5, np.pi, np.pi*1.5, 2 * np.pi], ymin=0, ymax=400, linestyles='dashed', colors='b', edgecolor = \"none\")\n",
    "    #plt.savefig(f\"{dir_name}samples_p{q}_theta_flow_prior_angles_{i}.pdf\", dpi=100)\n",
    "    plt.xlim(0, 2*np.pi)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T16:03:36.544967909Z",
     "start_time": "2023-12-04T16:03:35.802273036Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def lp_norm(arr, p):\n",
    "    norm = np.sum(np.power(np.abs(arr), p), 1)\n",
    "    norm = np.power(norm, 1/p).reshape(-1,1)\n",
    "    return arr/norm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:25:43.785774349Z",
     "start_time": "2023-09-06T12:25:43.746765362Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "%load_ext rpy2.ipython"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T12:25:44.249786449Z",
     "start_time": "2023-09-06T12:25:44.209252354Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "from rpy2.robjects.packages import importr\n",
    "rpgnorm = importr('pgnorm')\n",
    "q=1\n",
    "n_samples = 50000\n",
    "flow_dim = 10\n",
    "%R -i q -i flow_dim -i n_samples -o samples samples <- rpgnorm(n_samples * flow_dim, q)\n",
    "samples = np.array(samples).reshape(-1,flow_dim)\n",
    "samples_norm = lp_norm(samples, q)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:24:45.031632119Z",
     "start_time": "2023-09-06T15:24:43.792937656Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "samples_spherical = cartesian_to_spherical_torch(torch.from_numpy(samples_norm)).numpy()\n",
    "for i in range(1, samples_spherical.shape[1]):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    # n, bins, patches = plt.hist(samples_spherical[:,i], bins=200)\n",
    "    plt.hist(samples_spherical[:,i], bins=50, edgecolor = \"none\")\n",
    "    #plt.vlines([0, np.pi * 0.5, np.pi, np.pi*1.5, 2 * np.pi], ymin=0, ymax=max(n), linestyles='dashed', colors='b')\n",
    "    # plt.savefig(f\"{dir_name}samples_p{q}_theta_gt.pdf\", dpi=100)\n",
    "    plt.xlim(0,2*np.pi)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T15:24:47.735465675Z",
     "start_time": "2023-09-06T15:24:45.886410684Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cond_flow",
   "language": "python",
   "display_name": "cond_flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
